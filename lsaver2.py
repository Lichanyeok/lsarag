#pip install -U langchain openai faiss-cpu tiktoken PyPDF2 beautifulsoup4 requests streamlit langchain-community pypdf python-dotenv google-api-python-client langchain-openai

# ì²­í‚¹
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = PyPDFLoader("./ê·¼ë¡œê¸°ì¤€ë²•.pdf")
pages = loader.load() #pagesì™€ pdfì˜ ì´ ì¥ìˆ˜ëŠ” ê°™ìŒ

cleaned_pages = [
    page.__class__(page.page_content.replace('\n', ' ').strip(), metadata=page.metadata)
    for page in pages
]

text_splitter = RecursiveCharacterTextSplitter(chunk_size =2000, chunk_overlap=500,)
document = text_splitter.split_documents(cleaned_pages)


# ì„ë² ë”©
from langchain.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv
from langchain.vectorstores import FAISS
import os

load_dotenv()

open_api_key = os.getenv('OPENAI_API_KEY')
embeddings = OpenAIEmbeddings()
faiss_vectorstore = FAISS.from_documents(document,embeddings)
faiss_vectorstore.save_local("./lsa_vectorstore")


# %%
# from langchain.chains import ConversationalRetrievalChain
# from langchain_openai import ChatOpenAI
# from langchain.prompts import PromptTemplate

# llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
# retriever = faiss_vectorstore.as_retriever(k=3)

# prompt = PromptTemplate(
#   input_variables=["context","question"],
#   template="""
#     ë‹¹ì‹ ì€ ê·¼ë¡œê¸°ì¤€ë²•ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë²•ì  ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ëŠ” ì „ë¬¸ ë²•ë¥ ê°€ì…ë‹ˆë‹¤.
#     ì•„ë˜ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ ë¬¸ì„œì—ì„œ ì·¨ë“í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ questionì— ì ì ˆí•œ ë‹µë³€ì„ ë¶€íƒí•©ë‹ˆë‹¤.
#     ---
#     {context}
#     ---
#     ë‹¨, ì§ˆë¬¸ì´ ìœ„ì˜ ê·¼ë¡œê¸°ì¤€ë²• ë‚´ìš©ê³¼ ê´€ë ¨ì´ ì—†ì„ ì‹œ "í•´ë‹¹ ì§ˆë¬¸ì€ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ ë‚´ìš©ì´ ì•„ë‹™ë‹ˆë‹¤." ë¼ëŠ” ë‹µë³€ìœ¼ë¡œ í†µì¼í•´ì¤˜.
#     ê·¼ë¡œê¸°ì¤€ë²•ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì¼ ê²½ìš° ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•í•˜ê²Œ ì œì‹œí•œ ë‹µë³€ì„ ê°„ëµí•˜ê²Œ ìš”ì•½í•˜ì—¬ ì „ë‹¬í•´ì¤˜.
#     ---
#     question:{question} 
#   """
# )

# # rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
# chat_chain = ConversationalRetrievalChain.from_llm(
#   llm = llm,
#   retriever = retriever,
#   condense_question_prompt=prompt,
#   verbose=True
# )

# chat_chain({"question":"ì¿ ë²„ë„¤í‹°ìŠ¤ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì¤˜","chat_history":[]})
# '''
# ì˜ˆìƒ ë‹µë³€ 
# {'question': 'ì¿ ë²„ë„¤í‹°ìŠ¤ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì¤˜',
#  'chat_history': [],
#  'answer': 'í•´ë‹¹ ì§ˆë¬¸ì€ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ ë‚´ìš©ì´ ì•„ë‹™ë‹ˆë‹¤. 
#  ì‹¤ì œ ë‹µë³€
#  {'question': 'ì¿ ë²„ë„¤í‹°ìŠ¤ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì¤˜',
#  'chat_history': [],
#  'answer': 'ì£„ì†¡í•˜ì§€ë§Œ, ì¿ ë²„ë„¤í‹°ìŠ¤ì˜ ì‚¬ìš©ë²•ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¿ ë²„ë„¤í‹°ìŠ¤ëŠ” ì»¨í…Œì´ë„ˆí™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°°í¬, í™•ì¥ ë° ê´€ë¦¬ë¥¼ ìë™í™”í•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤. ì‚¬ìš©ë²•ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” ê³µì‹ ë¬¸ì„œë‚˜ ê´€ë ¨ íŠœí† ë¦¬ì–¼ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ê³µì‹ ë¬¸ì„œëŠ” [Kubernetes ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://kubernetes.io/docs/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'

#  ì™œ ì˜ˆìƒí•œëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ê°€ ì‘ë™í•˜ì§€ ì•Šì•˜ì„ê¹Œ ? 
#  - ConversationalRetrievalChainì²´ì¸ì˜ ì‘ë™ êµ¬ì¡°ë¥¼ ì•Œì•„ì•¼í•¨. condense_question_promptì— ë§¤í•‘í•œ í”„ë¡¬í”„íŠ¸ëŠ” ì™„ì„±í˜• ë‹µë³€ì„ ì£¼ëŠ”ê²ƒì— ì ìš© í•˜ëŠ” í”„ë¡¬í”„íŠ¸ê°€ ì•„ë‹ˆë¼.
#   ì±„íŒ…ì˜ ë¶ˆì™„ì „í•œ ì§ˆë¬¸ì„ ë³´ì™„í•  ë•Œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì„. ë”°ë¼ì„œ í•´ë‹¹ í”„ë¡¬í”„íŠ¸ëŠ” context(ì„ë² ë”©í•´ì„œ ë“¤ê³ ì˜¨ ë¬¸ì„œ)ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ê²Œ ë¨ -> ë‹µë³€ì„ ì™„ì„±ì‹œí‚¬ ë•Œ ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸.
#   ë”°ë¼ì„œ ì²´ì¸ì„ ê²°í•©í•˜ëŠ” ì‹ìœ¼ë¡œ êµ¬ì„±í•´ì•¼ ì˜ë„í•œ ëŒ€ë¡œ ì‘ë™í•˜ê²Œ ë¨
# '''


# í”„ë¡¬í”„íŠ¸ ë‚´ë¶€êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²´ì¸ ì„¤ê³„
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain,ConversationalRetrievalChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.prompts import PromptTemplate
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
retriever = faiss_vectorstore.as_retriever(k=3)

#ë¶ˆì™„ì „í•œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸
question_prompt = PromptTemplate(
  input_variables=["question","chat_history"],
  template="""
  ë‹¤ìŒ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”:
  ëŒ€í™” : {chat_history}
  ì§ˆë¬¸ : {question}
"""
)

#generator ìƒì„±
question_generator = LLMChain(llm=llm, prompt=question_prompt)

#ì›í•˜ëŠ” ë‹µë³€ì˜ ìµœì¢… í…œí”Œë¦¿
custom_answer_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    ë‹¹ì‹ ì€ ê·¼ë¡œê¸°ì¤€ë²•ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë²•ì  ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ëŠ” ì „ë¬¸ ë²•ë¥ ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ ë¬¸ì„œì—ì„œ ì·¨ë“í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ questionì— ì ì ˆí•œ ë‹µë³€ì„ ë¶€íƒí•©ë‹ˆë‹¤.
    ---
    {context}
    ---
    ë‹¨, ë„Œ ê·¼ë¡œê¸°ì¤€ë²•ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì—ë§Œ ë‹µë³€ì„ í•  ì˜ë¬´ê°€ ìˆì–´.
    ì§ˆë¬¸ì´ ê·¼ë¡œê¸°ì¤€ë²• ë‚´ìš©ê³¼ ì™„ì „íˆ ê´€ë ¨ ì—†ì„ ì‹œ "í•´ë‹¹ ì§ˆë¬¸ì€ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ ë‚´ìš©ì´ ì•„ë‹™ë‹ˆë‹¤." ë¼ëŠ” ë‹µë³€ìœ¼ë¡œ í†µì¼í•´ì¤˜. ì˜ˆë¥¼ ë“¤ë©´, "ìŒì‹ì˜ ë ˆì‹œí”¼ë¥¼ ì§ˆë¬¸í•˜ê±°ë‚˜, ìœ í–‰í•˜ëŠ” ë…¸ë˜ë¥¼ ë¬»ëŠ”ë‹¤ê±°ë‚˜, íŠ¹ì • ì¸ë¬¼ì— ëŒ€í•´ ì§ˆë¬¸ í•˜ëŠ” ê²ƒ ì²˜ëŸ¼ ë§ì´ì•¼.
    í•˜ì§€ë§Œ, ìœ„ì˜ ê·¼ë¡œê¸°ì¤€ë²• ë‚´ìš©ê³¼ ì¡°ê¸ˆ ì¼ì¹˜í•˜ì§€ ì•Šë”ë¼ë„ ë…¸ë™ í˜¹ì€ ê·¼ë¬´ì— ê´€í•œ ë‚´ìš©ì€ ì°¾ì•„ì„œ ë‹µë³€í•´ì¤˜
    ëª¨ë“  ë‹µë³€ì€ ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ê³  ê°„ëµí•˜ê²Œ ìš”ì•½í•˜ì—¬ ì „ë‹¬í•´ì¤˜.
    ---
    ì§ˆë¬¸: {question}
    """)
    #contextì™€ ê´€ë ¨ì´ ì—†ë”ë¼ë„ ê·¼ë¡œê¸°ì¤€ì— ê´€í•˜ì—¬ ë‹µë³€ ìœ ë„ ë˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
    #question : ìµœì €ì„ê¸ˆ ê¸°ì¤€ ì£¼ 60ì‹œê°„ ê·¼ë¬´ ì‹œ ì£¼íœ´ìˆ˜ë‹¹ì€ ì–¼ë§ˆì•¼ ?
    #asis -> í•´ë‹¹ ì§ˆë¬¸ì€ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ ë‚´ìš©ì´ ì•„ë‹™ë‹ˆë‹¤.
    #tobe -> 
    # ì£¼íœ´ìˆ˜ë‹¹ì€ ê·¼ë¡œìê°€ 1ì£¼ ë™ì•ˆ ì†Œì •ì˜ ê·¼ë¡œì¼ì„ ê°œê·¼í•œ ê²½ìš°ì— ì§€ê¸‰ë˜ëŠ” ìœ ê¸‰íœ´ì¼ì— ëŒ€í•œ ìˆ˜ë‹¹ì…ë‹ˆë‹¤. ì£¼íœ´ìˆ˜ë‹¹ì€ 1ì£¼ì¼ ë™ì•ˆì˜ ì†Œì •ê·¼ë¡œì‹œê°„ì— ëŒ€í•œ ì„ê¸ˆì„ ê¸°ì¤€ìœ¼ë¡œ ì‚°ì •ë©ë‹ˆë‹¤.
    # ì£¼íœ´ìˆ˜ë‹¹ ê³„ì‚° ë°©ë²•:

    # ì£¼íœ´ìˆ˜ë‹¹ = (1ì£¼ ì†Œì •ê·¼ë¡œì‹œê°„ / 40ì‹œê°„) Ã— 8ì‹œê°„ Ã— ì‹œê¸‰
    # ì£¼ 60ì‹œê°„ ê·¼ë¬´ ì‹œ:

    # ì£¼ 60ì‹œê°„ ê·¼ë¬´ëŠ” ì—°ì¥ê·¼ë¡œê°€ í¬í•¨ëœ ì‹œê°„ì…ë‹ˆë‹¤. ì£¼íœ´ìˆ˜ë‹¹ì€ ì†Œì •ê·¼ë¡œì‹œê°„(ê¸°ë³¸ ê·¼ë¡œì‹œê°„)ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°ë©ë‹ˆë‹¤.
    # ì†Œì •ê·¼ë¡œì‹œê°„ì´ ì£¼ 40ì‹œê°„ì´ë¼ë©´, ì£¼íœ´ìˆ˜ë‹¹ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:
    # ì£¼íœ´ìˆ˜ë‹¹ = (40ì‹œê°„ / 40ì‹œê°„) Ã— 8ì‹œê°„ Ã— ìµœì €ì‹œê¸‰
    # ìµœì €ì„ê¸ˆ ê¸°ì¤€:

    # 2023ë…„ ê¸°ì¤€ ìµœì €ì‹œê¸‰ì´ 9,620ì›ì´ë¼ë©´:
    # ì£¼íœ´ìˆ˜ë‹¹ = 8ì‹œê°„ Ã— 9,620ì› = 76,960ì›
    



qa_llm_chain = LLMChain(llm=llm, prompt=custom_answer_prompt)
combine_docs_chain = StuffDocumentsChain(
    llm_chain=qa_llm_chain,
    document_variable_name="context"
)


#ìµœì¢… ì²´ì¸ êµ¬ì„±
chat_chain = ConversationalRetrievalChain(
  retriever = retriever,
  question_generator = question_generator,
  combine_docs_chain = combine_docs_chain,
  verbose = True
)

# google ê²€ìƒ‰ ì²´ì¸
from langchain.tools import Tool
from langchain.utilities import GoogleSearchAPIWrapper
from dotenv import load_dotenv

load_dotenv()
search = GoogleSearchAPIWrapper()

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

summary_prompt = PromptTemplate(
  input_variables=["content"],
  template=""" ë‹¤ìŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì˜ í•µì‹¬ì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ê³ , ë²•ì  ì‚¬ë¡€ì¸ì§€, ë‰´ìŠ¤ì¸ì§€ êµ¬ë¶„í•´ì„œ ìš”ì•½í•´ì¤˜:
    {content}
  """
)

summary_chain = LLMChain(llm=llm,prompt=summary_prompt)



# ìŠ¤íŠ¸ë¦¼ë¦¿ ë„ì›Œì„œ ì²´ì¸ run
import streamlit as st

st.title("ê·¼ë¡œê¸°ì¤€ë²• ì±—ë´‡")
query = st.text_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        result = chat_chain({"question": query, "chat_history":st.session_state.chat_history})
        st.session_state.chat_history.append((query, result["answer"]))
        
        st.subheader("ğŸ“˜ ë²•ë ¹ ê¸°ë°˜ ë‹µë³€")
        st.write(result["answer"])

        st.subheader("ğŸ’¬ ì´ì „ ëŒ€í™” ê¸°ë¡")
        for user_q, bot_a in st.session_state.chat_history:
            st.markdown(f"ì‚¬ìš©ì ğŸš€ : {user_q}")
            st.markdown(f"ë‹µë³€ :sunglasses: : {bot_a}")
        # ì™¸ë¶€ ì‚¬ë¡€ ê²€ìƒ‰ ë° ìš”ì•½
        search_results = search.results(query=query + " íŒë¡€", num_results=1)
        if not search_results[0]['Result']:
            web_content = search_results[0]['snippet']
            summary = summary_chain.run(content=web_content)
            st.subheader("ğŸ“Œ ì‹¤ì œ ì‚¬ë¡€ ìš”ì•½")
            st.write(summary)
            st.markdown(f"ğŸ”— [ì¶œì²˜ ë§í¬]({search_results[0]['link']})")

        


