
# %%
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = PyPDFLoader("./ê·¼ë¡œê¸°ì¤€ë²•.pdf")
pages = loader.load() #pagesì™€ pdfì˜ ì´ ì¥ìˆ˜ëŠ” ê°™ìŒ

cleaned_pages = [
    page.__class__(page.page_content.replace('\n', ' ').strip(), metadata=page.metadata)
    for page in pages
]

text_splitter = RecursiveCharacterTextSplitter(chunk_size =2000, chunk_overlap=500,)
document = text_splitter.split_documents(cleaned_pages)


# %%
from langchain.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv
from langchain.vectorstores import FAISS
import os

load_dotenv()

open_api_key = os.getenv('OPENAI_API_KEY')
embeddings = OpenAIEmbeddings()
faiss_vectorstore = FAISS.from_documents(document,embeddings)
faiss_vectorstore.save_local("./lsa_vectorstore")

# %%
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
retriever = faiss_vectorstore.as_retriever(k=3)
rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)


# %%
from langchain.tools import Tool
from langchain.utilities import GoogleSearchAPIWrapper
from dotenv import load_dotenv

load_dotenv()
search = GoogleSearchAPIWrapper()

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

summary_prompt = PromptTemplate(
  input_variables=["content"],
  template=""" ë‹¤ìŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì˜ í•µì‹¬ì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ê³ , ë²•ì  ì‚¬ë¡€ì¸ì§€, ë‰´ìŠ¤ì¸ì§€ êµ¬ë¶„í•´ì„œ ìš”ì•½í•´ì¤˜:
    {content}
  """
)

summary_chain = LLMChain(llm=llm,prompt=summary_prompt)



# %%
import streamlit as st

st.title("ê·¼ë¡œê¸°ì¤€ë²• ì±—ë´‡")
query = st.text_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        rag_result = rag_chain(query)
        st.subheader("ğŸ“˜ ë²•ë ¹ ê¸°ë°˜ ë‹µë³€")
        st.write(rag_result["result"])

        # ì™¸ë¶€ ì‚¬ë¡€ ê²€ìƒ‰ ë° ìš”ì•½
        search_results = search.results(query=query, num_results=1)
        try: 
            web_content = search_results[0]['snippet']
            summary = summary_chain.run(content=web_content)
            st.subheader("ğŸ“Œ ì‹¤ì œ ì‚¬ë¡€ ìš”ì•½")
            st.write(summary)
            st.markdown(f"ğŸ”— [ì¶œì²˜ ë§í¬]({search_results[0]['link']})")
        except:
            st.subheader("ğŸ“Œ ì¡°íšŒëœ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤")

